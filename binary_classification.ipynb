{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/spacy/util.py:833: UserWarning: [W095] Model 'en_core_web_sm' (3.0.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.2.3). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from rake_nltk import Rake\n",
    "rakers = Rake()\n",
    "\n",
    "import re \n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import scipy\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn import model_selection, preprocessing, feature_selection, ensemble, linear_model, metrics, decomposition\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "cv = CountVectorizer(max_df=0.95, min_df=2, stop_words=\"english\")\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "model = Doc2Vec.load(\"models/cosine_doc2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(text, stem=\"None\"):\n",
    "\n",
    "    final_string = \"\"\n",
    "\n",
    "    # Make lower\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove line breaks\n",
    "    text = re.sub('\\n', '', text)\n",
    "\n",
    "    # Remove puncuation\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    # Remove stop words\n",
    "    text = text.split()\n",
    "    useless_words = nltk.corpus.stopwords.words(\"english\")\n",
    "    useless_words = useless_words + ['hi', 'im']\n",
    "\n",
    "    text_filtered = [word for word in text if not word in useless_words]\n",
    "\n",
    "    # Remove numbers\n",
    "    text_filtered = [re.sub('\\w*\\d\\w*', '', w) for w in text_filtered]\n",
    "\n",
    "    # Stem or Lemmatize\n",
    "    if stem == 'Stem':\n",
    "        stemmer = PorterStemmer() \n",
    "        text_stemmed = [stemmer.stem(y) for y in text_filtered]\n",
    "    elif stem == 'Lem':\n",
    "        lem = WordNetLemmatizer()\n",
    "        text_stemmed = [lem.lemmatize(y) for y in text_filtered]\n",
    "    elif stem == 'Spacy':\n",
    "        text_filtered = nlp(' '.join(text_filtered))\n",
    "        text_stemmed = [y.lemma_ for y in text_filtered]\n",
    "    else:\n",
    "        text_stemmed = text_filtered\n",
    "\n",
    "    final_string = ' '.join(text_stemmed)\n",
    "\n",
    "    return final_string\n",
    "\n",
    "# combo['cleaned'] = combo['excerpt'].apply(lambda x: clean_string(x, stem='Stem'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data \n",
    "\n",
    "# read = pd.read_csv('https://raw.githubusercontent.com/joshnicholas/article_reccomendation/main/archive/read.csv')\n",
    "# unread = pd.read_csv('https://github.com/joshnicholas/article_reccomendation/blob/main/archive/unread.csv?raw=true')\n",
    "\n",
    "vectored = pd.read_csv('archive/vectored.csv')\n",
    "\n",
    "# print(vectored.columns)\n",
    "# 'status', 'resolved_title', 'resolved_url', 'excerpt', 'cleaned',\n",
    "#        'card2vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['think donald trump', 'wing influencers', 'raising questions', 'political discussion', 'electoral success', '’', 'whether', 'translate', 'time', 'spend', 'right', 'november', 'november', 'need', 'listen', 'liberals', 'get', 'facebook', 'facebook', 'elected', 'dominating']\n",
      "Right-wing influencers are dominating the political discussion on Facebook, raising questions about whether it will translate into electoral success in November. Listen, liberals. If you don’t think Donald Trump can get re-elected in November, you need to spend more time on Facebook.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Clean excerpts\n",
    "\n",
    "# combo['cleaned'] = combo['cleaned'].astype(str)\n",
    "\n",
    "# card2vec = [model.infer_vector(combo.iloc[i]['cleaned'].split(' '))\n",
    "#             for i in range(0,len(combo))]\n",
    "\n",
    "\n",
    "### Create new features to help with machine learning\n",
    "\n",
    "vectored['title_count'] = vectored['resolved_title'].str.split(\" \").str.len()\n",
    "\n",
    "vectored = vectored[:1]\n",
    "\n",
    "\n",
    "listo = []\n",
    "for index, row in vectored.iterrows():\n",
    "\n",
    "    texto = row['card2vec'].replace(\"[\", '').replace(\"]\", '').split(\",\")\n",
    "\n",
    "    status =  row['status']\n",
    "    titlo = row['resolved_title']\n",
    "    title_count = row['title_count']\n",
    "    urlo = row['resolved_url']\n",
    "\n",
    "    excerpt = row['excerpt']\n",
    "\n",
    "    a = rakers.extract_keywords_from_text(excerpt)\n",
    "    keywords = rakers.get_ranked_phrases()\n",
    "\n",
    "    data = [{'resolved_title': titlo,\n",
    "    'resolved_url': urlo,\n",
    "    'keywords':  }]\n",
    "\n",
    "    inter = pd.DataFrame.from_records(data)\n",
    "\n",
    "    listo.append(inter)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "p = combo\n",
    "\n",
    "# print(p)\n",
    "# print(p.columns)\n",
    "# print(p['title_count'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read = vectored.loc[vectored['status'] == 1].copy()\n",
    "unread = vectored.loc[vectored['status'] == 0].copy()\n",
    "\n",
    "### Sample the data to get 700 of each \n",
    "\n",
    "read = read.sample(n=7000)\n",
    "unread = unread.sample(n=7000)\n",
    "\n",
    "read = read[['status', 'resolved_title', 'excerpt', 'word_count']]\n",
    "unread = unread[['status', 'resolved_title', 'excerpt', 'word_count']]\n",
    "\n",
    "combo = unread.append(read)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
